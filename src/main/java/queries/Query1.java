package queries;
import utils.CsvWriter;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.util.StatCounter;
import scala.Tuple2;
import scala.Tuple5;
import utils.QueriesPreprocessing;
import java.time.Duration;
import java.time.Instant;
import java.time.LocalDateTime;
import java.time.OffsetDateTime;

public class Query1 {

    public static void query1Main(JavaRDD<String> rdd) {

        // (tpep_pickup_datetime, payment_type, tip_amount, tolls_amount, tot_amount)
        JavaRDD<Tuple5<LocalDateTime, Double, Double, Double, Double>> rddPreproc = QueriesPreprocessing.Query1Preprocessing(rdd);

        Instant start = Instant.now();

        // resultRDD : (2021-12, (mean, count))
        JavaPairRDD<String, Tuple2<Double, Long>> resultRDD = computeResults(rddPreproc);

        Instant end = Instant.now();

        System.out.println("Durata query1 : " + Duration.between(start,end).toMillis());

        for (Tuple2<String, Tuple2<Double, Long>> s : resultRDD.collect()) {
            System.out.println(s);
        }

        CsvWriter.writeQuery1HDFS_CSV(resultRDD);

        }

      
    private static JavaPairRDD<String, Tuple2<Double, Long>> computeResults(JavaRDD<Tuple5<LocalDateTime, Double, Double, Double, Double>> rdd) {

        // (month, tip_amount/(total_amount - tolls_amount)
        JavaPairRDD<String, Double> rddAvgTip = rdd.mapToPair(
                word -> {
                    LocalDateTime odt = word._1();
                    String key = odt.getYear() + "-" + odt.getMonthValue();
                    Double value = word._3() / (word._5()- word._4());
                    return new Tuple2<>(key, value);
                });


        // (month, (mean, occurrences))
        JavaPairRDD<String, Tuple2<Double, Long>> output = rddAvgTip
                .filter(x -> !(Double.isNaN(x._2()))) //remove NaN values generated bytip/(total amount - toll amount) (for example, 0.0/0.0 = NaN)
                .aggregateByKey(
                        new StatCounter(),
                        StatCounter::merge,
                        StatCounter::merge)
                .mapToPair(x -> new Tuple2<>(x._1(), new Tuple2<>(x._2().mean(), x._2().count())))
                .sortByKey();
        return output;
    }
}
